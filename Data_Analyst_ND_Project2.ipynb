{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the NYC Subway Dataset\n",
    "\n",
    "Craig Nicholson\n",
    "September 9th 2015\n",
    "\n",
    "In this notebook, we will review look the NYC Subway data and determine if more people ride the subway when it is raining versus when it is not raining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels as sm\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# display plot within the notebook rather than a new window\n",
    "%pylab inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the stroopdata.csv\n",
    "path = '~/Downloads/P2-1/turnstile_weather_v2.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fetch two samples, the one with rain and the one without rain\n",
    "# 1 = rain, 0 = no rain for the entry\n",
    "rain = df[df['rain']==1]\n",
    "norain =  df[df['rain']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Statistical Test\n",
    "\n",
    "The mean difference of people ride the subway when it is raining versus when it is not raining.\n",
    "\n",
    "#### null Hypothesis\n",
    "H0: μd = μ0\n",
    "\n",
    "\n",
    "#### Alternate Hypothesis\n",
    "HA: μd ≠ μ0\n",
    "\n",
    "#### Statistical Test\n",
    "Since we have one large sample and are reviewing riders on the days it is raining vs. not raining we will run an Independant T-test.  And the ENTRIESn_hourly occurs over a date range.\n",
    "\n",
    "#### Independant variables\n",
    "Rain vs No Rain\n",
    "\n",
    "#### Dependent variable\n",
    "The number of ENTRIESn_hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNIT</th>\n",
       "      <th>DATEn</th>\n",
       "      <th>TIMEn</th>\n",
       "      <th>ENTRIESn</th>\n",
       "      <th>EXITSn</th>\n",
       "      <th>ENTRIESn_hourly</th>\n",
       "      <th>EXITSn_hourly</th>\n",
       "      <th>datetime</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_week</th>\n",
       "      <th>...</th>\n",
       "      <th>pressurei</th>\n",
       "      <th>rain</th>\n",
       "      <th>tempi</th>\n",
       "      <th>wspdi</th>\n",
       "      <th>meanprecipi</th>\n",
       "      <th>meanpressurei</th>\n",
       "      <th>meantempi</th>\n",
       "      <th>meanwspdi</th>\n",
       "      <th>weather_lat</th>\n",
       "      <th>weather_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>R003</td>\n",
       "      <td>05-04-11</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>4391507</td>\n",
       "      <td>2913223</td>\n",
       "      <td>83</td>\n",
       "      <td>174</td>\n",
       "      <td>2011-05-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>29.94</td>\n",
       "      <td>1</td>\n",
       "      <td>59.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>29.953333</td>\n",
       "      <td>53.933333</td>\n",
       "      <td>11.7</td>\n",
       "      <td>40.700348</td>\n",
       "      <td>-73.887177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>R003</td>\n",
       "      <td>05-04-11</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>4391531</td>\n",
       "      <td>2913258</td>\n",
       "      <td>24</td>\n",
       "      <td>35</td>\n",
       "      <td>2011-05-04 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>29.91</td>\n",
       "      <td>1</td>\n",
       "      <td>60.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>29.953333</td>\n",
       "      <td>53.933333</td>\n",
       "      <td>11.7</td>\n",
       "      <td>40.700348</td>\n",
       "      <td>-73.887177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>R003</td>\n",
       "      <td>05-04-11</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>4392063</td>\n",
       "      <td>2913388</td>\n",
       "      <td>532</td>\n",
       "      <td>130</td>\n",
       "      <td>2011-05-04 08:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>29.94</td>\n",
       "      <td>1</td>\n",
       "      <td>55.4</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>29.953333</td>\n",
       "      <td>53.933333</td>\n",
       "      <td>11.7</td>\n",
       "      <td>40.700348</td>\n",
       "      <td>-73.887177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>R003</td>\n",
       "      <td>05-04-11</td>\n",
       "      <td>12:00:00</td>\n",
       "      <td>4392517</td>\n",
       "      <td>2913495</td>\n",
       "      <td>454</td>\n",
       "      <td>107</td>\n",
       "      <td>2011-05-04 12:00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>29.96</td>\n",
       "      <td>1</td>\n",
       "      <td>51.1</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>29.953333</td>\n",
       "      <td>53.933333</td>\n",
       "      <td>11.7</td>\n",
       "      <td>40.700348</td>\n",
       "      <td>-73.887177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>R003</td>\n",
       "      <td>05-04-11</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>4392764</td>\n",
       "      <td>2913744</td>\n",
       "      <td>247</td>\n",
       "      <td>249</td>\n",
       "      <td>2011-05-04 16:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>29.96</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>29.953333</td>\n",
       "      <td>53.933333</td>\n",
       "      <td>11.7</td>\n",
       "      <td>40.700348</td>\n",
       "      <td>-73.887177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    UNIT     DATEn     TIMEn  ENTRIESn   EXITSn  ENTRIESn_hourly  \\\n",
       "16  R003  05-04-11  00:00:00   4391507  2913223               83   \n",
       "17  R003  05-04-11  04:00:00   4391531  2913258               24   \n",
       "18  R003  05-04-11  08:00:00   4392063  2913388              532   \n",
       "19  R003  05-04-11  12:00:00   4392517  2913495              454   \n",
       "20  R003  05-04-11  16:00:00   4392764  2913744              247   \n",
       "\n",
       "    EXITSn_hourly             datetime  hour  day_week     ...       \\\n",
       "16            174  2011-05-04 00:00:00     0         2     ...        \n",
       "17             35  2011-05-04 04:00:00     4         2     ...        \n",
       "18            130  2011-05-04 08:00:00     8         2     ...        \n",
       "19            107  2011-05-04 12:00:00    12         2     ...        \n",
       "20            249  2011-05-04 16:00:00    16         2     ...        \n",
       "\n",
       "    pressurei rain  tempi  wspdi meanprecipi  meanpressurei  meantempi  \\\n",
       "16      29.94    1   59.0    9.2        0.01      29.953333  53.933333   \n",
       "17      29.91    1   60.1    0.0        0.01      29.953333  53.933333   \n",
       "18      29.94    1   55.4   15.0        0.01      29.953333  53.933333   \n",
       "19      29.96    1   51.1   11.5        0.01      29.953333  53.933333   \n",
       "20      29.96    1   48.0   18.4        0.01      29.953333  53.933333   \n",
       "\n",
       "    meanwspdi  weather_lat  weather_lon  \n",
       "16       11.7    40.700348   -73.887177  \n",
       "17       11.7    40.700348   -73.887177  \n",
       "18       11.7    40.700348   -73.887177  \n",
       "19       11.7    40.700348   -73.887177  \n",
       "20       11.7    40.700348   -73.887177  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "any mising dates  \n",
    "are all dates valid  \n",
    "any missing ENTRIESn_hourly  \n",
    "any missing hours, any hours out of the correct date?  \n",
    "we have day_week can we use this for the weekend   \n",
    "do we have folks using more UNITs in other areas more than others...   \n",
    "what happens on the weekend with the data...  \n",
    "when it rains does it rain for all hours in the day or just like early in the AM...\n",
    "what is the hourly entries like throughout the day... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# t-test via scipy  - run this first , and then review and calculate to learn the math\n",
    "# returns t-statistic and two-tailed p-value\n",
    "t_test_results = stats.ttest_ind(norain['ENTRIESn_hourly'],rain['ENTRIESn_hourly'],equal_var = False)\n",
    "t_statistic = t_test_results[0]\n",
    "p_value = t_test_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the (n) number of samples in each \n",
    "rain_n = rain['UNIT'].count() \n",
    "norain_n = norain['UNIT'].count() \n",
    "\n",
    "# Get the means and medians for the rain\n",
    "rain_xbar= rain['ENTRIESn_hourly'].mean()\n",
    "rain_median = rain['ENTRIESn_hourly'].median()\n",
    "\n",
    "# Get the means and medians for the norain\n",
    "norain_xbar = norain['ENTRIESn_hourly'].mean()\n",
    "norain_median = norain['ENTRIESn_hourly'].median()\n",
    "\n",
    "# Get the standard deviations of the samples\n",
    "# http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html\n",
    "rain_sd = stats.tstd(rain['ENTRIESn_hourly'], limits=None, inclusive=(True, True))\n",
    "norain_sd = stats.tstd(norain['ENTRIESn_hourly'], limits=None, inclusive=(True, True))\n",
    "\n",
    "# Standard Error of the Mean\n",
    "# I need to run this calculations myself and valiate the independant test\n",
    "rain_se = stats.sem(rain['ENTRIESn_hourly'], axis=None, ddof=1)\n",
    "norain_se = stats.sem(norain['ENTRIESn_hourly'], axis=None, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the t-critical value, instead of using a table - woohoo\n",
    "# http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.t.html\n",
    "# By adding 1 to 0.95 and then dividing by 2, we get the critical value for 2 tail test\n",
    "degrees_of_freedom = (rain_n + norain_n) - 2\n",
    "critical_value = stats.t._ppf((1+0.95)/2., degrees_of_freedom)\n",
    "critical_value = stats.t._ppf((1+0.99)/2., degrees_of_freedom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate Sum of Squares (SS for rain), pooled var, pooled SE\n",
    "# (X1 - XBar)**2\n",
    "rain_SS = np.sum((rain['ENTRIESn_hourly']-rain_xbar)**2)\n",
    "norain_SS = np.sum((norain['ENTRIESn_hourly']-norain_xbar)**2)\n",
    "rain_SS, norain_SS\n",
    "pooled_varience = (rain_SS+norain_SS)/((rain_n-1)+(norain_n-1))\n",
    "pooled_SD = (rain_SS+norain_SS) / ((norain_n-1)+(rain_n-1))\n",
    "pooled_SE = np.sqrt(pooled_varience/rain_n + pooled_varience/norain_n)\n",
    "t_stat_check = (rain_xbar-norain_n)/pooled_SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rain_small = rain['ENTRIESn_hourly']\n",
    "#rain_small['diff_mean_var']  = rain['ENTRIESn_hourly']-rain_xbar\n",
    "#rain_small['xbar'] = rain_xbar\n",
    "#rain_small['diff'] = rain_small['ENTRIESn_hourly'] - rain_small['xbar']\n",
    "#rain_small['diff_mean_var'].head(5)\n",
    "#rain_small['SS'] = np.square(rain_small['diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cohens D\n",
    "cohens_d = (rain_xbar-norain_xbar)/pooled_SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Confidence Intervals\n",
    "lower_CI = 0\n",
    "upper_CI = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESCRIPTIVE STATISTICS\n",
      "\t\tn \t\tmean \t\tmedian \t\tStDev \t\tSE Mean\n",
      "Rain   \t\t9585.00 \t2028.20 \t939.00 \t\t3189.43 \t32.58\n",
      "No Rain\t\t33064.00 \t1845.54 \t893.00 \t\t2878.77 \t15.83\n",
      "\n",
      "\n",
      "We need pooled varience: 8710971.755\n",
      "We need pooled standard deviation: 8710971.755\n",
      "We need pooled standard error 34.238\n",
      "\n",
      "\n",
      "INFERENTIAL STATISTICS\n",
      "Type of test\t\t\t = Independent t-test\n",
      "Test statistics: T \t\t = -5.04288\n",
      "Degrees of Freedom\t\t = 42648\n",
      "Significance Level: alpha \t = 0.01 \n",
      "Critical value: t1-α/2,ν \t = 2.58\n",
      "Critical region: Reject H0 if \t = |T| > 2.58\n",
      "P-Value\t\t\t\t = 0.000000464\n",
      "Direction of test\t\t = two tail \n",
      "\n",
      "\n",
      "CONFIDENCE INTERVALS\n",
      "95% CI for the mean difference: (0.00000, 0.00000)\n",
      "Margin of Error: 0.00\n",
      "\n",
      "\n",
      "EFFECT SIZE MEASURES\n",
      "r squared \t\t\t = 0.00060\n",
      "Cohens-D\t\t\t = 0.00002\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'DESCRIPTIVE STATISTICS'\n",
    "print '\\t\\tn \\t\\tmean \\t\\tmedian \\t\\tStDev \\t\\tSE Mean'\n",
    "print 'Rain   \\t\\t%.2f \\t%.2f \\t%.2f \\t\\t%.2f \\t%.2f' % (rain_n,rain_xbar, rain_median,rain_sd,rain_se)\n",
    "print 'No Rain\\t\\t%.2f \\t%.2f \\t%.2f \\t\\t%.2f \\t%.2f' % (norain_n,norain_xbar,norain_median,norain_sd,norain_se)\n",
    "print '\\n'\n",
    "print 'We need pooled varience: %.3f' % pooled_varience\n",
    "print 'We need pooled standard deviation: %.3f' % pooled_SD\n",
    "print 'We need pooled standard error %.3f' % pooled_SE\n",
    "print '\\n'\n",
    "\n",
    "print 'INFERENTIAL STATISTICS'\n",
    "# Paired t-test, two tail\n",
    "# t-statistic\n",
    "# df \n",
    "# p-value\n",
    "# direction of test (two tail, or one tail to in _/- directrion')\n",
    "print 'Type of test\\t\\t\\t = %s' % 'Independent t-test'\n",
    "print 'Test statistics: T \\t\\t = %.5f' % t_statistic\n",
    "print 'Degrees of Freedom\\t\\t = %.0f' % ((rain_n+norain_n)-1)\n",
    "print 'Significance Level: alpha \\t = %s ' % '0.01'\n",
    "print 'Critical value: t1-α/2,ν \\t = %.2f' % critical_value\n",
    "print 'Critical region: Reject H0 if \\t = |T| > %.2f' % critical_value\n",
    "print 'P-Value\\t\\t\\t\\t = %.9f' % p_value\n",
    "print 'Direction of test\\t\\t = %s ' % 'two tail'\n",
    "print '\\n'\n",
    "\n",
    "print 'CONFIDENCE INTERVALS'\n",
    "print '95%% CI for the mean difference: (%.5f, %.5f)' % (lower_CI,upper_CI)\n",
    "print 'Margin of Error: %.2f' % ((upper_CI-lower_CI)/2.0)\n",
    "print '\\n'\n",
    "\n",
    "print 'EFFECT SIZE MEASURES'\n",
    "# r - ratio - X:Y, \n",
    "r_squared = ( np.square(t_statistic) /  (np.square(t_statistic) + (degrees_of_freedom)  ))\n",
    "print 'r squared \\t\\t\\t = %.5f' % r_squared \n",
    "print 'Cohens-D\\t\\t\\t = %.5f' % cohens_d\n",
    "print '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Which statistical test did you use to analyze the NYC subway data? Did you use a one-tail or a two-tail P value? What is the null hypothesis? What is your p-critical value?\n",
    "\n",
    "The independent-samples t-test (or independent t-test, for short) compares the means between two unrelated groups on the same continuous, dependent variable.\n",
    "\n",
    "The independent t-test, also called the two sample t-test or student's t-test, is an inferential statistical test that determines whether there is a statistically significant difference between the means in two unrelated groups.\n",
    "\n",
    "The null hypothesis for the independent t-test is that the population means from the two unrelated groups are equal:\n",
    "\n",
    "H0: u1 = u2\n",
    "\n",
    "In most cases, we are looking to see if we can show that we can reject the null hypothesis and accept the alternative hypothesis, which is that the population means are not equal:\n",
    "\n",
    "HA: u1 ≠ u2\n",
    "\n",
    "To do this, we need to set a significance level (alpha) that allows us to either reject or accept the alternative hypothesis. Most commonly, this value is set at 0.05.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Why is this statistical test applicable to the dataset?\n",
    "\n",
    "We have two unrelated groups on the same continuous, dependent variable, with different sample sizes.\n",
    "\n",
    "###  In particular, consider the assumptions that the test is making about the distribution of ridership in the two samples.\n",
    "\n",
    "#### Assumption of normality of the dependent variable\n",
    "The independent t-test requires that the dependent variable is approximately normally distributed within each group. We can test for this using a multitude of tests, but the Shapiro-Wilks Test or a graphical method, such as a Q-Q Plot, are very common. You can run these tests using SPSS, the procedure for which can be found in our Testing for Normality guide. However, the t-test is described as a robust test with respect to the assumption of normality. This means that even deviations away from normality do not have a large influence on Type I error rates. The exception to this is if the difference in the size of the groups is greater than 1.5 (largest compared to smallest).\n",
    "\n",
    "RAIN IS 3.44 times smaller than no rain hourly entries.\n",
    "\n",
    "\n",
    "#### What to do when you violate the normality assumption\n",
    "If you find that either one or both of your group's data is not approximately normally distributed and groups sizes differ greatly, you have two options: (1) transform your data so that the data becomes normally distributed (to do this in SPSS see our guide on Transforming Data), or (2) run the Mann-Whitney U Test which is a non-parametric test that does not require the assumption of normality (to run this test in SPSS see our guide on the Mann-Whitney U Test).\n",
    "\n",
    "#### Assumption of Homogeneity of Variance\n",
    "The independent t-test assumes the variances of the two groups you are measuring to be equal. If your variances are unequal, this can affect the Type I error rate. The assumption of homogeneity of variance can be tested using Levene's Test of Equality of Variances, which is produced in SPSS when running the independent t-test. If you have run Levene's Test of Equality of Variances, whether in SPSS or by another means, you will get a result similar to that below:\n",
    "\n",
    "Levene's Test for Equality of Variances in the Independent T-Test Procedure within SPSS\n",
    "This test for homogeneity of variance provides an F statistic and a significance value (p-value). We are primarily concerned with the significance level - if it is greater than 0.05, our group variances can be treated as equal. However, if p < 0.05, we have unequal variances and we have violated the assumption of homogeneity of variance.\n",
    "\n",
    "#### Overcoming a Violation of the Assumption of Homogeneity of Variance\n",
    "If the Levene's Test for Equality of Variances is statistically significant, and therefore indicates unequal variances, we can correct for this violation by not using the pooled estimate for the error term for the t-statistic, and also making adjustments to the degrees of freedom using the Welch-Satterthwaite method. In all reality, you will probably never have heard of these adjustments as SPSS hides this information and simply labels the two options as \"Equal variances assumed\" and \"Equal variances not assumed\" without explicitly stating the underlying tests used. However, you can see the evidence of these tests as below:\n",
    "\n",
    "Differences in the t-statistic and the degrees of freedom when homogeneity of variance is not assumed\n",
    "From the result of Levene's Test for Equality of Variances, we can reject the null hypothesis that there is no difference in the variances between the groups and accept the alternative hypothesis that there is a significant difference in the variances between groups. The effect of not being able to assume equal variances is evident in the final column of the above figure where we see a reduction in the value of the t-statistic and a large reduction in the degrees of freedom (df). This has the effect of increasing the p-value above the critical significance level of 0.05. In this case, we therefore do not accept the alternative hypothesis and accept that there are no statistically significant differences between means. This would not have been our conclusion had we not tested for homogeneity of variances.\n",
    "\n",
    "\n",
    "https://statistics.laerd.com/statistical-guides/independent-t-test-statistical-guide.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption #1: Your dependent variable should be measured on a continuous scale (i.e., it is measured at the interval or ratio level). Examples of variables that meet this criterion include revision time (measured in hours), intelligence (measured using IQ score), exam performance (measured from 0 to 100), weight (measured in kg), and so forth. You can learn more about continuous variables in our article: Types of Variable.\n",
    "\n",
    "Assumption #2: Your independent variable should consist of two categorical, independent groups. Example independent variables that meet this criterion include gender (2 groups: male or female), employment status (2 groups: employed or unemployed), smoker (2 groups: yes or no), and so forth.\n",
    "\n",
    "Assumption #3: You should have independence of observations, which means that there is no relationship between the observations in each group or between the groups themselves. For example, there must be different participants in each group with no participant being in more than one group. This is more of a study design issue than something you can test for, but it is an important assumption of the independent t-test. If your study fails this assumption, you will need to use another statistical test instead of the independent t-test (e.g., a paired-samples t-test). If you are unsure whether your study meets this assumption, you can use our Statistical Test Selector, which is part of our enhanced content.\n",
    "\n",
    "Assumption #4: There should be no significant outliers. Outliers are simply single data points within your data that do not follow the usual pattern (e.g., in a study of 100 students' IQ scores, where the mean score was 108 with only a small variation between students, one student had a score of 156, which is very unusual, and may even put her in the top 1% of IQ scores globally). The problem with outliers is that they can have a negative effect on the independent t-test, reducing the validity of your results. Fortunately, when using SPSS to run an independent t-test on your data, you can easily detect possible outliers. In our enhanced independent t-test guide, we: (a) show you how to detect outliers using SPSS; and (b) discuss some of the options you have in order to deal with outliers. You can learn more about our enhanced independent t-test guide here.\n",
    "\n",
    "Assumption #5: Your dependent variable should be approximately normally distributed for each group of the independent variable. We talk about the independent t-test only requiring approximately normal data because it is quite \"robust\" to violations of normality, meaning that this assumption can be a little violated and still provide valid results. You can test for normality using the Shapiro-Wilk test of normality, which is easily tested for using SPSS. In addition to showing you how to do this in our enhanced independent t-test guide, we also explain what you can do if your data fails this assumption (i.e., if it fails it more than a little bit). Again, you can learn more here.\n",
    "\n",
    "Assumption #6: There needs to be homogeneity of variances. You can test this assumption in SPSS using Levene’s test for homogeneity of variances. In our enhanced independent t-test guide, we (a) show you how to perform Levene’s test for homogeneity of variances in SPSS, (b) explain some of the things you will need to consider when interpreting your data, and (c) present possible ways to continue with your analysis if your data fails to meet this assumption (learn more here).\n",
    "\n",
    "\n",
    "You can check assumptions #4, #5 and #6 using SPSS. Before doing this, you should make sure that your data meets assumptions #1, #2 and #3, although you don't need SPSS to do this. When moving on to assumptions #4, #5 and #6, we suggest testing them in this order because it represents an order where, if a violation to the assumption is not correctable, you will no longer be able to use an independent t-test (although you may be able to run another statistical test on your data instead). Just remember that if you do not run the statistical tests on these assumptions correctly, the results you get when running an independent t-test might not be valid. This is why we dedicate a number of sections of our enhanced independent t-test guide to help you get this right. You can find out about our enhanced independent t-test guide here, or more generally, our enhanced content as a whole here.\n",
    "\n",
    "In the section, Test Procedure in SPSS, we illustrate the SPSS procedure required to perform an independent t-test assuming that no assumptions have been violated. First, we set out the example we use to explain the independent t-test procedure in SPSS.\n",
    "\n",
    "\n",
    "\n",
    "https://statistics.laerd.com/spss-tutorials/independent-t-test-using-spss-statistics.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 What results did you get from this statistical test? These should include the following numerical values: p-values, as well as the means for each of the two samples under test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 What is the significance and interpretation of these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What approach did you use to compute the coefficients theta and produce prediction for ENTRIESn_hourly in your regression model: OLS using Statsmodels or Scikit Learn Gradient descent using Scikit Learn Or something different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 What features (input variables) did you use in your model? Did you use any dummy variables as part of your features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Why did you select these features in your model? We are looking for specific reasons that lead you to believe that the selected features will contribute to the predictive power of your model. Your reasons might be based on intuition. For example, response for fog might be: “I decided to use fog because I thought that when it is very foggy outside people might decide to use the subway more often.” Your reasons might also be based on data exploration and experimentation, for example: “I used feature X because as soon as I included it in my model, it drastically improved my R2 value.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 What are the parameters (also known as \"coefficients\" or \"weights\") of the non-dummy features in your linear regression model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 What is your model’s R2 (coefficients of determination) value? 2.6 What does this R2 value mean for the goodness of fit for your regression model? Do you think this linear model to predict ridership is appropriate for this dataset, given this R2 value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Visualization\n",
    "\n",
    "Please include two visualizations that show the relationships between two or more variables in the NYC subway data.  \n",
    "\n",
    "Remember to add appropriate titles and axes labels to your plots. Also, please add a short description below each figure commenting on the key insights depicted in the figure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d82edd8ce5df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hour'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'violet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Efficiency in PPPC\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Frequency\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cnicholson/anaconda/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mhist\u001b[1;34m(x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, hold, **kwargs)\u001b[0m\n\u001b[0;32m   2894\u001b[0m                       \u001b[0mhisttype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhisttype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2895\u001b[0m                       \u001b[0mrwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2896\u001b[1;33m                       stacked=stacked, **kwargs)\n\u001b[0m\u001b[0;32m   2897\u001b[0m         \u001b[0mdraw_if_interactive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cnicholson/anaconda/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mhist\u001b[1;34m(self, x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[0;32m   5602\u001b[0m         \u001b[1;31m# Massage 'x' for processing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5603\u001b[0m         \u001b[1;31m# NOTE: Be sure any changes here is also done below to 'weights'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5604\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5605\u001b[0m             \u001b[1;31m# TODO: support masked arrays;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5606\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cnicholson/anaconda/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cnicholson/anaconda/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   1593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1595\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3113)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:2844)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3704)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:7224)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:7162)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b57868150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hack = rain['ENTRIESn_hourly']\n",
    "#plt.scatter(x=rain['datetime'], y=rain['ENTRIESn_hourly'])\n",
    "#plt.xlabel(\"Length in mm\")\n",
    "#plt.ylabel(\"Efficiency in PPPC\")\n",
    "#plt.title(\"Average Food Pinching Efficiency by Chopstick Length\")\n",
    "#plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(x=rain['hour'], bins=15, color='violet')\n",
    "plt.xlabel(\"Efficiency in PPPC\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Average & Median Food Pinching Efficiency \\nby Chopstick Length 240mm\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 One visualization should contain two histograms: one of  ENTRIESn_hourly for rainy days and one of ENTRIESn_hourly for non-rainy days.\n",
    "You can combine the two histograms in a single plot or you can use two separate plots.\n",
    "If you decide to use to two separate plots for the two histograms, please ensure that the x-axis limits for both of the plots are identical. It is much easier to compare the two in that case.\n",
    "\n",
    "\n",
    "For the histograms, you should have intervals representing the volume of ridership (value of ENTRIESn_hourly) on the x-axis and the frequency of occurrence on the y-axis. For example, each interval (along the x-axis), the height of the bar for this interval will represent the number of records (rows in our data) that have ENTRIESn_hourly that falls in this interval.\n",
    "Remember to increase the number of bins in the histogram (by having larger number of bars). The default bin width is not sufficient to capture the variability in the two samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 One visualization can be more freeform. \n",
    "\n",
    "You should feel free to implement something that we discussed in class (e.g., scatter plots, line plots) or attempt to implement something more advanced if you'd like. Some suggestions are:\n",
    "Ridership by time-of-day\n",
    "Ridership by day-of-week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Conclusion\n",
    "\n",
    "Please address the following questions in detail. Your answers should be 1-2 paragraphs long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 From your analysis and interpretation of the data, do more people ride the NYC subway when it is raining or when it is not raining?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 What analyses lead you to this conclusion? You should use results from both your statistical tests and your linear regression to support your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5. Reflection\n",
    "\n",
    "Please address the following questions in detail. Your answers should be 1-2 paragraphs long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Please discuss potential shortcomings of the methods of your analysis, including: Dataset, Analysis, such as the linear regression model or statistical test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Please discuss potential shortcomings of the methods of your analysis, including: Dataset, Analysis, such as the linear regression model or statistical test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referenecs\n",
    "\n",
    "Python 3.4.2 Documentation  Retrieved from https://docs.python.org/\n",
    "\n",
    "ggplot from yhat Retrieved from http://ggplot.yhathq.com/\n",
    "\n",
    "SciPy.org Retrieved from http://scipy.org/\n",
    "\n",
    "Engineering Statistics Retrieved from http://www.itl.nist.gov/div898/handbook/\n",
    "\n",
    "5.2.4.  Are the model residuals well-behaved? [Blog Post] Retrieved from http://www.itl.nist.gov/div898/handbook/pri/section2/pri24.htm\n",
    "\n",
    "The Minitab Blog What Is the Difference between Linear and Nonlinear Equations in Regression Analysis? [Blog Post] Retrieved from http://blog.minitab.com/blog/adventures-in-statistics \n",
    "\n",
    "The Minitab Blog Regression Analysis how do I interpret R Squared and assess the goodness of fit. [Blog Post] Retrieved from http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit\n",
    "\n",
    "What are the differences between one-tailed and two-tailed tests? [Blog Post] Retrieved from http://www.ats.ucla.edu/stat/mult_pkg/faq/general/tail_tests.htm\n",
    "\n",
    "Dummy Variables [Wiki Post] Retrieved from http://en.wikipedia.org/wiki/Dummy_variable_(statistics) \n",
    "\n",
    "Mann-Whitney U Test using SPSS [Blog Post] Retrieved from https://statistics.laerd.com/spss-tutorials/mann-whitney-u-test-using-spss-statistics.phpRegression \n",
    "\n",
    "Analysis – How to Interpret the Constant Y Intercept [Blog Post] Retrieved from http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-to-interpret-the-constant-y-intercept \n",
    "\n",
    "Coding in the Rain [Blog Post] http://drjasondavis.com/2013/07/08/coding-in-the-rain/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
